
# Transformer 之 位置编码（Position Encoding，PE）


----

[toc]

----

在这篇博客中，我将深入浅出地介绍深度学习中位置编码的原理，帮助你轻松理解这个重要概念。


!["1"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/1.webp)


在深度学习的广袤领域中，**位置编码（Position Encoding）**是一个关键概念，尤其是在处理序列数据或具有空间结构的数据时。它解决了深度学习模型中一个基础但又至关重要的问题：**如何让模型感知数据中的位置信息?**


----


## 一、为什么需要位置编码


!["2"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/2.png)


以**自然语言处理（NLP）**为例，当我们处理一段文本时，单词的顺序是传达语义的关键。“我喜欢苹果”和“苹果喜欢我”，虽然单词相同，但由于顺序不同，语义完全不同。然而，许多深度学习模型，如自注意力机制（Self-Attention）本身并没有内置的位置感知能力。`自注意力机制在计算时，主要关注的是元素之间的相关性，而不考虑元素在序列中的位置`。这就好比一个人在听故事时，只关注每个情节的内容，却不关心情节发生的先后顺序，这样显然无法完整理解故事的含义。

在**图像处理**中，图像中的像素位置同样包含着重要信息。比如，一张猫的图片，猫的眼睛、鼻子、嘴巴在图像中的位置决定了这是一张猫的图像，而不是其他动物的图像。如果模型不能感知这些像素的位置，就无法准确识别图像中的物体。因此，为了`让深度学习模型能够有效地处理这些具有顺序或空间结构的数据`，我们需要引入位置编码，将位置信息融入到模型的输入中。

----

## 二、常见的位置编码方式
### 2.1 正弦和余弦位置编码（Sinusoidal Positional Encoding）
这是在2027年的论文[《Transformer：Attention Is All You Need》](https://arxiv.org/abs/1706.03762)中提出的一种经典位置编码方式。其核心思想是`利用正弦和余弦函数的周期性来对位置进行编码`。

#### 2.1.1 公式
对于一个长度为$L$的序列，每个位置$pos$（$pos = 0, 1, ..., L - 1$）都要被编码到一个$d$维的向量空间中。位置编码向量的第$index$维元素计算方式如下：
$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$
$$
PE(pos, 2i + 1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$
其中，$2i$表示偶数维度，$2i + 1$表示奇数维度。 这里需要注意，$i$`不是直接是生成的位置编码的`$index$，$2i$和$2i + 1$分别是$index$的偶数和奇数部分。

可以看到，这里通过不同的频率来区分不同的维度。随着维度的增加，频率的变化越来越缓慢。这种设计使得模型能够学习到不同位置之间的相对距离关系。因为正弦和余弦函数的周期性，当两个位置之间的距离固定时，它们在位置编码向量上的差异也是固定的，这有助于模型理解序列中元素的相对位置。


#### 2.1.2 例子
举个简单的例子，假设我们有一个长度为5的序列，使用5维的位置编码。**对于位置0，其位置编码向量为**：
$$
PE(0, 0) = \sin\left(\frac{0}{10000^{0/5}}\right) = 0
$$
$$
PE(0, 1) = \cos\left(\frac{0}{10000^{0/5}}\right) = 1
$$
$$
PE(0, 2) = \sin\left(\frac{0}{10000^{2/5}}\right) = 0
$$
$$
PE(0, 3) = \cos\left(\frac{0}{10000^{2/5}}\right) = 1
$$
$$
PE(0, 4) = \sin\left(\frac{0}{10000^{4/5}}\right) = 0
$$

**对于位置1，其位置编码向量为**：
$$
PE(1, 0) = \sin\left(\frac{1}{10000^{0/5}}\right) \approx 0.001
$$
$$
PE(1, 1) = \cos\left(\frac{1}{10000^{0/5}}\right) \approx 0.999
$$
$$
PE(1, 2) = \sin\left(\frac{1}{10000^{2/5}}\right) \approx 0.001
$$
$$
PE(1, 3) = \cos\left(\frac{1}{10000^{2/5}}\right) \approx 0.999
$$
$$
PE(1, 4) = \sin\left(\frac{1}{10000^{4/5}}\right) \approx 0.001
$$

可以看到，随着位置的变化，位置编码向量的值也在有规律地变化，这种变化反映了位置之间的差异。


#### 2.1.3 可视化
只看上面的公式还不够直观，下面我们结合具体正余弦曲线详细说明。当 $d = 50$ 时，

**A. 索引$index$**
!["2-1"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/2-2.png)

!["2-2"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/2-3.png)


 - **第0和第1个索引**：如上图所示，在三角函数 $\sin$ 和 $\cos$ 中，当涉及到第0和第1个索引时，有特殊处理。因为 $index = 0$ 和 $index = 1$ 时， $i = 0$，  $10000^{0/50}=10000^0 = 1$  。所以 $\sin(\frac{pos}{10000^{0/50}})$ 就等同于 $\sin(pos)$  ， $\cos(\frac{pos}{10000^{0/50}})$ 等同于 $\cos(pos)$  。此时，`第0和第1个index索引的位置编码值仅由 `$pos$ `（位置信息）决定，而不涉及模型维度相关的` $d$ 。
 - **第2个索引起**：从第2个索引（ $index\geq2$  ）开始，$i\geq1$， $10000^{2i/50}$  中，由于 $2i$ 不再为0， $10000^{2i/50}> 1$  ，除数变大，计算方式和前两个索引不同，`位置编码值的计算不再是简单依赖` $pos$  ，计算逻辑变得更复杂，和模型维度等因素关联起来 ，而且由于index 的增加，正弦余弦的曲线更加平滑，如上图所示。
 - **后面的索引**：当索引进一步增大，（如 $index = 48$  时， $2i = 48$   ）， $\frac{pos}{10000^{48/50}} \approx \frac{pos}{10000}$  。根据三角函数的周期公式 $T = \frac{2\pi}{\omega}$  （对于 $y = A\sin(\omega x+\varphi)$  ），这里 $\omega=\frac{1}{10000}$  ，`计算出周期` $T = 20000\pi$  。由于这个周期远远超出了实际的 $pos$  值范围，所以大部分计算出的三角函数值会接近0或1 。这意味着在较大索引时，`位置编码值在一定程度上呈现出特殊的分布特性，集中在0或1附近` ，如下两图所示。


 !["2-3"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/2-4.png)


简单来说，就是在Transformer位置编码计算里，开头两个索引的三角函数计算因为指数运算结果特殊，相对简单。后续索引计算逻辑因指数变化而改变。最后索引对应的指相对稳定，因为正余弦曲线周期更长。 


**B. 纬度$d$**

下图是 $d=50$ 和 $d=20$时，对应的位置编码的正余弦曲线图，图中 $index$ 只有前面的[0,1,2,3]。可以发现 $index$=0或1时，$d=50$ 和 $d=20$对应的正余弦曲线是一样的，都是sin(pos)和cos(pos)。当 $index\geq2$ 时，$d=20$对应的曲线明显比$d=50$平滑。也就是周期更长。

 !["4"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/4.png)

下图展示了完整$d=50$时，生成的前20个pos的位置编码向量。（行对应pos，列对应d，也就是index）。前面列的部分0到20的列变化频率较高，而第 30-50 列的颜色几乎没有变化。

 !["3"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/3.png)






下图展示了制数万个位置的值，其中还是$d=50$。因为 pos 增多后整个特征的变化频率看起来更加高了，但其实上图是下图的最上面二十个pos的对应的位置向量。


 !["5"](https://gitee.com/ss20210321/data_storage/raw/master/Note_images/Position%20Encoder/5.png)

（此图有内置错觉，因为它试图在 670px（高度）上打印 40k+ 值，所以它无法显示波长小于 1px 的任何值的正确值。这就是为什么即使使用正确的值来生成此图，第 24 列之前的所有内容在视觉上都是不正确的。）







-----



### 2.2 基于学习的绝对位置编码（Learnable Absolute Positional Encoding）
与正弦和余弦位置编码不同，基于学习的绝对位置编码是通过模型学习得到的。具体来说，它定义了两个可学习的嵌入层（Embedding），分别对图像的行和列位置（在NLP中可以理解为序列中的位置）进行编码。

对于图像中高度为$h$和宽度为$w$的位置，令行位置编码为$y_i$，列位置编码为$x_j$，则最终的二维位置编码为：
$$
PE_{(i,j)}=(x_j,y_i) \in \mathbb{R}^{2\times d}
$$
其中$d$是嵌入维度。在实现中，列编码$x_j$和行编码$y_i$分别由嵌入层计算：$x_j = Embedding(j)$，$y_i = Embedding(i)$。最终通过扩展和拼接生成一个形状为$(B, 2d, h, w)$的四维位置编码张量，其中$B$是批量大小。

这种方式的灵活性在于位置嵌入能够随着数据分布的优化而调整。模型可以根据训练数据中的位置信息模式，自动学习到最适合的位置编码表示。在NLP任务中，如果训练数据中某些位置的单词具有特定的语义倾向，模型通过学习位置编码可以更好地捕捉这种关系。



-----

### 2.3 相对位置编码



----



### 2.4 旋转位置编码（RoPE - Rotary Position Embedding）

[RoPE（Rotary Position Embedding，旋转位置编码）](https://arxiv.org/abs/2104.09864)是一种用于深度学习模型的位置编码技术，由苏剑林等人在2021年提出，核心思想是`通过旋转变换将位置信息融入token的表示中`，使模型能够捕捉序列中的相对位置关系。以下是其核心细节与技术优势：


#### 2.4.1 RoPE的核心技术原理  
##### A. 从绝对位置到相对位置的突破  
传统位置编码（如BERT的绝对位置编码）`仅为每个token分配唯一编码`，无法直接建模token间的相对距离（如`第5个词与第10个词的关系`）。  
RoPE通过**复数域旋转变换**，将位置信息嵌入到Attention机制中，使模型在计算注意力时自然包含相对位置信息。具体来说：  
- 对每个token的查询向量  $Q$  和键向量  $K$ ，根据其位置  $i$  和  $j$  进行旋转缩放：  
  $$
  Q_i = Q_i \cdot \cos(\theta_j) + Q_i^\perp \cdot \sin(\theta_j)  
  $$  
  $$
  K_j = K_j \cdot \cos(\theta_i) + K_j^\perp \cdot \sin(\theta_i)  
  $$  
  其中  $\theta_j = 10000^{-2d/d_{\text{model}}}$  是可学习的频率参数， $Q_i^\perp$  是  $Q_i$  的`正交向量`。  

##### B. 旋转操作的数学本质  
RoPE`利用三角函数的正交性，将位置信息转化为向量空间中的旋转角度`。当计算  $Q_i$  和  $K_j$  的内积时，相对位置  $|i-j|$  会通过旋转角度差影响注意力权重，从而实现“位置敏感”的注意力：  
$$
\text{Attention}(Q, K) = \text{softmax}\left(\frac{(Q \cdot \text{RoPE}(i))(K \cdot \text{RoPE}(j))^T}{\sqrt{d}}\right)
$$  
这种设计让模型无需额外计算即可捕捉长距离依赖，且对序列长度不敏感。


#### 2.4.2 RoPE的核心优势（对比传统位置编码）  
| **优势**                | **具体表现**                                                                 |  
|-------------------------|-----------------------------------------------------------------------------|  
| **相对位置建模**        | 直接编码token间的相对距离，解决绝对位置编码在长序列中的位置混淆问题（如“第100词与第200词的关系”）。 |  
| **长序列处理能力**      | 旋转操作的周期性避免了位置信息随序列长度增加而衰减，在处理4K+ tokens时性能优于传统编码（如LLaMA-3使用RoPE支持8K上下文）。 |  
| **多维输入适配**        | 可扩展至二维/三维数据（如图像、视频），通过分解位置维度（如高度、宽度、时间）实现跨模态位置编码（如Qwen2.5-Omni的TMRoPE）。 |  
| **与Attention深度融合**  | 位置信息通过乘法嵌入Q/K向量，而非简单相加，强化位置信号在深层网络中的传播（传统编码易被激活函数稀释）。 |  


RoPE通过数学上的巧妙设计，将位置信息深度融入注意力机制，解决了传统编码的两大痛点：长序列衰减和相对位置建模。其可扩展性（从文本到多模态）和高效性（适配深层网络）使其成为LLaMA、Qwen等顶尖模型的首选位置编码方案。特别是在[《Qwen2.5-Omni》](https://arxiv.org/abs/2503.20215) 和 [《Qwen2.5 VL》](https://arxiv.org/abs/2502.13923) 中，RoPE被扩展为TMRoPE 和 M-RoPE，进一步融合时间维度，标志着位置编码技术从“文本专用”向“全模态通用”的跨越。



----


## 三、位置编码的应用场景
位置编码在许多深度学习任务中都有广泛应用。在**自然语言处理**中，除了前面提到的文本理解任务，还在机器翻译、文本生成等任务中发挥着关键作用。在机器翻译中，源语言句子和目标语言句子的单词顺序需要准确对应，位置编码帮助模型理解这种顺序关系，从而提高翻译的准确性。在文本生成任务中，位置编码使得生成的文本具有合理的语法和语义顺序。

在**计算机视觉**领域，位置编码在图像分类、目标检测、语义分割等任务中也得到了应用。在图像分类中，位置编码可以帮助模型更好地理解图像中不同区域的特征与物体类别的关系。在目标检测任务中，位置编码有助于模型确定目标物体在图像中的位置。在语义分割中，位置编码可以让模型更准确地分割出不同物体的边界。


-----


## 总结
位置编码是深度学习中一个不可或缺的技术，它为模型提供了感知数据中位置信息的能力。不同的位置编码方式，如正弦和余弦位置编码、基于学习的绝对位置编码、旋转位置编码等，各有其特点和优势。在实际应用中，我们需要根据具体的任务和数据特点选择合适的位置编码方式，以提升模型的性能。随着深度学习的不断发展，位置编码技术也在不断演进，未来有望出现更高效、更强大的位置编码方法，为深度学习在各个领域的应用带来新的突破。






## 引用


[1]. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

[2]. [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)

[3]. [Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215)

[4]. [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)

[5]. [Understanding Positional Encoding in Transformers](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)


[6]. [BERT输入以及权重矩阵形状解析](https://blog.csdn.net/weixin_44305115/article/details/130788785)


